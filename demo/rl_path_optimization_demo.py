"""
å¼ºåŒ–å­¦ä¹ è·¯å¾„ä¼˜åŒ– Demo

è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„demoï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ Gymnasium å’Œ Stable-Baselines3 
æ¥å®ç°è·¯å¾„ä¼˜åŒ–ã€‚

æ³¨æ„ï¼šè¿™ä¸ªdemoä¸ä¼šä¿®æ”¹ä½ çš„é¡¹ç›®ä»£ç ï¼Œå¯ä»¥ç‹¬ç«‹è¿è¡Œã€‚
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„ï¼ˆç”¨äºå¯¼å…¥å·¥å…·å‡½æ•°ï¼‰
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šä½¿ç”¨ Gymnasium å®šä¹‰ç¯å¢ƒ ====================

try:
    import gymnasium as gym
    from gymnasium import spaces
    GYMNASIUM_AVAILABLE = True
except ImportError:
    print("âš ï¸  Gymnasium æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install gymnasium")
    GYMNASIUM_AVAILABLE = False

try:
    from stable_baselines3 import SAC, PPO
    from stable_baselines3.common.callbacks import EvalCallback
    SB3_AVAILABLE = True
except ImportError:
    print("âš ï¸  Stable-Baselines3 æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install stable-baselines3[extra]")
    SB3_AVAILABLE = False


class SimplePathEnv(gym.Env if GYMNASIUM_AVAILABLE else object):
    """
    ç®€åŒ–çš„è·¯å¾„ä¼˜åŒ–ç¯å¢ƒï¼ˆ2Dç‰ˆæœ¬ï¼Œä¾¿äºå¯è§†åŒ–ï¼‰
    
    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„demoï¼Œå±•ç¤ºRLè·¯å¾„ä¼˜åŒ–çš„åŸºæœ¬æ¦‚å¿µã€‚
    å®é™…é¡¹ç›®ä¸­åº”è¯¥æ˜¯3Dç¯å¢ƒã€‚
    
    ç»§æ‰¿è‡ª gymnasium.Env ä»¥å…¼å®¹ Stable-Baselines3
    """
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 4}
    
    def __init__(
        self,
        grid_size: Tuple[int, int] = (50, 50),
        start: Tuple[float, float] = (5.0, 5.0),
        goal: Tuple[float, float] = (45.0, 45.0),
        obstacle_map: Optional[np.ndarray] = None,
        render_mode: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–ç¯å¢ƒ
        
        Args:
            grid_size: æ …æ ¼å¤§å° (width, height)
            start: èµ·ç‚¹åæ ‡
            goal: ç»ˆç‚¹åæ ‡
            obstacle_map: éšœç¢ç‰©åœ°å›¾ï¼ˆå¯é€‰ï¼‰
            render_mode: æ¸²æŸ“æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
        """
        if GYMNASIUM_AVAILABLE:
            super().__init__()
        
        self.grid_size = grid_size
        self.start = np.array(start, dtype=np.float32)
        self.goal = np.array(goal, dtype=np.float32)
        self.render_mode = render_mode
        
        # åˆ›å»ºéšœç¢ç‰©åœ°å›¾ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
        if obstacle_map is None:
            self.obstacle_map = self._create_default_obstacles()
        else:
            self.obstacle_map = obstacle_map
        
        # å½“å‰çŠ¶æ€
        self.current_pos = self.start.copy()
        self.path = [self.start.copy()]
        self.step_count = 0
        self.max_steps = 500
        
        # å®šä¹‰åŠ¨ä½œç©ºé—´ï¼ˆè¿ç»­ï¼šæ–¹å‘ + æ­¥é•¿ï¼‰
        # åŠ¨ä½œï¼š[dx, dy, step_size]
        # dx, dy: æ–¹å‘ï¼ˆå½’ä¸€åŒ–åˆ°[-1, 1]ï¼‰
        # step_size: æ­¥é•¿ [0.1, 2.0]
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0, 0.1], dtype=np.float32),
            high=np.array([1.0, 1.0, 2.0], dtype=np.float32),
            dtype=np.float32
        )
        
        # å®šä¹‰çŠ¶æ€ç©ºé—´
        # çŠ¶æ€ï¼š[current_x, current_y, goal_x, goal_y, 
        #        distance_to_goal, min_obstacle_distance, 
        #        path_length, step_count_normalized]
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(8,),
            dtype=np.float32
        )
    
    def _create_default_obstacles(self) -> np.ndarray:
        """åˆ›å»ºé»˜è®¤éšœç¢ç‰©ï¼ˆä¸€ä¸ªçŸ©å½¢éšœç¢ç‰©ï¼‰"""
        obstacle_map = np.zeros(self.grid_size, dtype=bool)
        
        # åœ¨ä¸­é—´åˆ›å»ºä¸€ä¸ªçŸ©å½¢éšœç¢ç‰©
        x1, y1 = 15, 15
        x2, y2 = 35, 35
        obstacle_map[y1:y2, x1:x2] = True
        
        return obstacle_map
    
    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:
        """é‡ç½®ç¯å¢ƒ"""
        if GYMNASIUM_AVAILABLE:
            super().reset(seed=seed)
        
        if seed is not None:
            np.random.seed(seed)
        
        self.current_pos = self.start.copy()
        self.path = [self.start.copy()]
        self.step_count = 0
        
        observation = self._get_observation()
        info = {}
        
        return observation, info
    
    def step(self, action) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """
        æ‰§è¡ŒåŠ¨ä½œ
        
        Args:
            action: [dx, dy, step_size] æˆ– numpy array
        
        Returns:
            observation, reward, terminated, truncated, info
        """
        # ç¡®ä¿ action æ˜¯ numpy array
        if not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # è§£æåŠ¨ä½œ
        direction = action[:2]
        step_size = action[2] if len(action) > 2 else 1.0
        
        # å½’ä¸€åŒ–æ–¹å‘
        direction_norm = np.linalg.norm(direction)
        if direction_norm > 1e-6:
            direction = direction / direction_norm
        
        # è®¡ç®—ä¸‹ä¸€ä¸ªä½ç½®
        next_pos = self.current_pos + direction * step_size
        
        # æ£€æŸ¥è¾¹ç•Œ
        next_pos[0] = np.clip(next_pos[0], 0, self.grid_size[0] - 1)
        next_pos[1] = np.clip(next_pos[1], 0, self.grid_size[1] - 1)
        
        # æ£€æŸ¥ç¢°æ’
        collision = self._check_collision(next_pos)
        
        # æ›´æ–°çŠ¶æ€
        self.current_pos = next_pos
        self.path.append(next_pos.copy())
        self.step_count += 1
        
        # è®¡ç®—å¥–åŠ±
        reward = self._compute_reward(collision)
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        distance_to_goal = np.linalg.norm(self.current_pos - self.goal)
        reached_goal = distance_to_goal < 2.0
        terminated = reached_goal or collision
        truncated = self.step_count >= self.max_steps
        
        info = {
            'reached_goal': reached_goal,
            'collision': collision,
            'distance_to_goal': distance_to_goal,
        }
        
        observation = self._get_observation()
        return observation, reward, terminated, truncated, info
    
    def _check_collision(self, pos: np.ndarray) -> bool:
        """æ£€æŸ¥æ˜¯å¦ç¢°æ’éšœç¢ç‰©"""
        x, y = int(pos[0]), int(pos[1])
        if 0 <= x < self.grid_size[0] and 0 <= y < self.grid_size[1]:
            return self.obstacle_map[y, x]
        return True  # è¶…å‡ºè¾¹ç•Œè§†ä¸ºç¢°æ’
    
    def _get_observation(self) -> np.ndarray:
        """è·å–å½“å‰è§‚å¯Ÿï¼ˆçŠ¶æ€ï¼‰"""
        # åˆ°ç›®æ ‡çš„è·ç¦»
        distance_to_goal = np.linalg.norm(self.current_pos - self.goal)
        
        # åˆ°æœ€è¿‘éšœç¢ç‰©çš„è·ç¦»
        min_obstacle_dist = self._get_min_obstacle_distance(self.current_pos)
        
        # è·¯å¾„é•¿åº¦
        path_length = self._compute_path_length()
        
        # å½’ä¸€åŒ–æ­¥æ•°
        step_count_norm = self.step_count / self.max_steps
        
        # ç»„åˆçŠ¶æ€å‘é‡
        obs = np.array([
            self.current_pos[0] / self.grid_size[0],  # å½’ä¸€åŒ–x
            self.current_pos[1] / self.grid_size[1],  # å½’ä¸€åŒ–y
            self.goal[0] / self.grid_size[0],         # å½’ä¸€åŒ–ç›®æ ‡x
            self.goal[1] / self.grid_size[1],         # å½’ä¸€åŒ–ç›®æ ‡y
            distance_to_goal / 100.0,                 # å½’ä¸€åŒ–è·ç¦»
            min_obstacle_dist / 10.0,                 # å½’ä¸€åŒ–éšœç¢ç‰©è·ç¦»
            path_length / 200.0,                       # å½’ä¸€åŒ–è·¯å¾„é•¿åº¦
            step_count_norm,                          # å½’ä¸€åŒ–æ­¥æ•°
        ], dtype=np.float32)
        
        return obs
    
    def _get_min_obstacle_distance(self, pos: np.ndarray) -> float:
        """è®¡ç®—åˆ°æœ€è¿‘éšœç¢ç‰©çš„è·ç¦»"""
        x, y = int(pos[0]), int(pos[1])
        min_dist = float('inf')
        
        # æ£€æŸ¥å‘¨å›´åŒºåŸŸ
        search_radius = 10
        for dy in range(-search_radius, search_radius + 1):
            for dx in range(-search_radius, search_radius + 1):
                check_x, check_y = x + dx, y + dy
                if (0 <= check_x < self.grid_size[0] and 
                    0 <= check_y < self.grid_size[1] and 
                    self.obstacle_map[check_y, check_x]):
                    dist = np.sqrt(dx**2 + dy**2)
                    min_dist = min(min_dist, dist)
        
        return min_dist if min_dist < float('inf') else 20.0
    
    def _compute_path_length(self) -> float:
        """è®¡ç®—å½“å‰è·¯å¾„é•¿åº¦"""
        if len(self.path) < 2:
            return 0.0
        total_length = 0.0
        for i in range(1, len(self.path)):
            total_length += np.linalg.norm(self.path[i] - self.path[i-1])
        return total_length
    
    def _compute_reward(self, collision: bool) -> float:
        """è®¡ç®—å¥–åŠ±"""
        if collision:
            return -100.0  # ç¢°æ’å¤§æƒ©ç½š
        
        # åˆ°ç›®æ ‡çš„è·ç¦»
        distance_to_goal = np.linalg.norm(self.current_pos - self.goal)
        
        # åˆ°è¾¾ç›®æ ‡
        if distance_to_goal < 2.0:
            return 100.0
        
        # è·¯å¾„é•¿åº¦æƒ©ç½š
        step_length = np.linalg.norm(self.path[-1] - self.path[-2]) if len(self.path) > 1 else 0
        length_penalty = -0.1 * step_length
        
        # å®‰å…¨æ€§å¥–åŠ±ï¼ˆè·ç¦»éšœç¢ç‰©è¶Šè¿œè¶Šå¥½ï¼‰
        min_obstacle_dist = self._get_min_obstacle_distance(self.current_pos)
        safety_reward = 1.0 * min_obstacle_dist
        
        # è¿›åº¦å¥–åŠ±ï¼ˆå‘ç›®æ ‡ç§»åŠ¨ï¼‰
        if len(self.path) > 1:
            prev_distance = np.linalg.norm(self.path[-2] - self.goal)
            progress = prev_distance - distance_to_goal
            progress_reward = 0.5 * progress
        else:
            progress_reward = 0.0
        
        # å¹³æ»‘åº¦å¥–åŠ±ï¼ˆé¼“åŠ±å°è§’åº¦å˜åŒ–ï¼‰
        smoothness_reward = 0.0
        if len(self.path) > 2:
            v1 = self.path[-1] - self.path[-2]
            v2 = self.path[-2] - self.path[-3]
            if np.linalg.norm(v1) > 1e-6 and np.linalg.norm(v2) > 1e-6:
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                angle_change = np.arccos(np.clip(cos_angle, -1, 1))
                smoothness_reward = -0.5 * angle_change
        
        total_reward = length_penalty + safety_reward + progress_reward + smoothness_reward
        return total_reward
    
    def render(self, save_path: Optional[str] = None):
        """
        å¯è§†åŒ–ç¯å¢ƒ
        
        Args:
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æ˜¾ç¤ºå›¾åƒ
        """
        fig, ax = plt.subplots(figsize=(10, 10))
        
        # ç»˜åˆ¶éšœç¢ç‰©
        obstacle_vis = np.zeros((*self.grid_size, 3))
        obstacle_vis[self.obstacle_map] = [0.3, 0.3, 0.3]  # ç°è‰²éšœç¢ç‰©
        
        ax.imshow(obstacle_vis, origin='lower', extent=[0, self.grid_size[0], 0, self.grid_size[1]])
        
        # ç»˜åˆ¶èµ·ç‚¹
        ax.plot(self.start[0], self.start[1], 'go', markersize=15, label='Start', zorder=5)
        
        # ç»˜åˆ¶ç»ˆç‚¹
        ax.plot(self.goal[0], self.goal[1], 'ro', markersize=15, label='Goal', zorder=5)
        
        # ç»˜åˆ¶è·¯å¾„
        if len(self.path) > 1:
            path_array = np.array(self.path)
            ax.plot(path_array[:, 0], path_array[:, 1], 'b-', linewidth=2, label='Path', zorder=3)
            ax.plot(path_array[:, 0], path_array[:, 1], 'b.', markersize=5, zorder=4)
        
        # ç»˜åˆ¶å½“å‰ä½ç½®
        ax.plot(self.current_pos[0], self.current_pos[1], 'yo', markersize=10, label='Current', zorder=6)
        
        ax.set_xlim(0, self.grid_size[0])
        ax.set_ylim(0, self.grid_size[1])
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_title('Path Optimization Environment')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        if save_path:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            save_path_obj = Path(save_path)
            save_path_obj.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(str(save_path_obj), dpi=150, bbox_inches='tight')
            print(f"âœ… å›¾åƒå·²ä¿å­˜åˆ°: {save_path_obj.absolute()}")
        else:
            plt.show()
        
        plt.close()


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šä½¿ç”¨ Stable-Baselines3 è®­ç»ƒ ====================

def train_rl_agent(env: SimplePathEnv, algorithm: str = 'SAC', total_timesteps: int = 10000):
    """
    è®­ç»ƒRLæ™ºèƒ½ä½“
    
    Args:
        env: ç¯å¢ƒ
        algorithm: ç®—æ³•åç§° ('SAC' æˆ– 'PPO')
        total_timesteps: è®­ç»ƒæ­¥æ•°
    """
    if not SB3_AVAILABLE:
        print("âŒ Stable-Baselines3 æœªå®‰è£…ï¼Œæ— æ³•è®­ç»ƒ")
        return None
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {algorithm} æ™ºèƒ½ä½“...")
    print(f"   è®­ç»ƒæ­¥æ•°: {total_timesteps}")
    
    # é€‰æ‹©ç®—æ³•
    if algorithm == 'SAC':
        model = SAC(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=3e-4,
            buffer_size=100000,
            learning_starts=1000,
            batch_size=256,
        )
    elif algorithm == 'PPO':
        model = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
        )
    else:
        raise ValueError(f"æœªçŸ¥ç®—æ³•: {algorithm}")
    
    # è®­ç»ƒ
    model.learn(total_timesteps=total_timesteps)
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    return model


def test_agent(model, env: SimplePathEnv, demo_dir: Path, num_episodes: int = 3):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“"""
    print(f"\nğŸ§ª æµ‹è¯•æ™ºèƒ½ä½“ ({num_episodes} ä¸ªepisode)...")
    
    success_count = 0
    total_reward = 0
    
    for episode in range(num_episodes):
        obs, info = env.reset()
        episode_reward = 0
        done = False
        
        while not done:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            episode_reward += reward
        
        total_reward += episode_reward
        
        if info.get('reached_goal', False):
            success_count += 1
            print(f"  Episode {episode + 1}: âœ… æˆåŠŸåˆ°è¾¾ç›®æ ‡ï¼å¥–åŠ±: {episode_reward:.2f}")
        else:
            print(f"  Episode {episode + 1}: âŒ æœªåˆ°è¾¾ç›®æ ‡ã€‚å¥–åŠ±: {episode_reward:.2f}")
        
        # å¯è§†åŒ–æœ€åä¸€ä¸ªepisode
        if episode == num_episodes - 1:
            save_path = demo_dir / f"rl_path_result_episode_{episode + 1}.png"
            env.render(save_path=str(save_path))
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   æˆåŠŸç‡: {success_count}/{num_episodes} ({100*success_count/num_episodes:.1f}%)")
    print(f"   å¹³å‡å¥–åŠ±: {total_reward/num_episodes:.2f}")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¯¹æ¯”æ¼”ç¤º ====================

def random_path_demo(env: SimplePathEnv, demo_dir: Path):
    """éšæœºè·¯å¾„æ¼”ç¤ºï¼ˆå¯¹æ¯”ç”¨ï¼‰"""
    print("\nğŸ² éšæœºè·¯å¾„æ¼”ç¤ºï¼ˆå¯¹æ¯”ç”¨ï¼‰...")
    
    obs, info = env.reset()
    done = False
    
    while not done:
        # éšæœºåŠ¨ä½œ
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
    
    save_path = demo_dir / "rl_path_random.png"
    env.render(save_path=str(save_path))
    print(f"   æœ€ç»ˆè·ç¦»ç›®æ ‡: {info.get('distance_to_goal', 0):.2f}")


def greedy_path_demo(env: SimplePathEnv, demo_dir: Path):
    """è´ªå¿ƒè·¯å¾„æ¼”ç¤ºï¼ˆå¯¹æ¯”ç”¨ï¼‰"""
    print("\nğŸ¯ è´ªå¿ƒè·¯å¾„æ¼”ç¤ºï¼ˆå¯¹æ¯”ç”¨ï¼‰...")
    
    obs, info = env.reset()
    done = False
    
    while not done:
        # è´ªå¿ƒç­–ç•¥ï¼šç›´æ¥å‘ç›®æ ‡ç§»åŠ¨
        direction = env.goal - env.current_pos
        direction_norm = np.linalg.norm(direction)
        if direction_norm > 1e-6:
            direction = direction / direction_norm
        
        action = np.array([direction[0], direction[1], 1.0], dtype=np.float32)
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
    
    save_path = demo_dir / "rl_path_greedy.png"
    env.render(save_path=str(save_path))
    print(f"   æœ€ç»ˆè·ç¦»ç›®æ ‡: {info.get('distance_to_goal', 0):.2f}")


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„demo"""
    print("=" * 60)
    print("ğŸš€ å¼ºåŒ–å­¦ä¹ è·¯å¾„ä¼˜åŒ– Demo")
    print("=" * 60)
    
    # æ£€æŸ¥ä¾èµ–
    if not GYMNASIUM_AVAILABLE:
        print("\nâŒ è¯·å…ˆå®‰è£… Gymnasium: pip install gymnasium")
        return
    
    # åˆ›å»ºdemoç›®å½•
    demo_dir = Path(__file__).parent
    demo_dir.mkdir(exist_ok=True)
    
    # 1. åˆ›å»ºç¯å¢ƒ
    print("\nğŸ“¦ åˆ›å»ºç¯å¢ƒ...")
    env = SimplePathEnv(
        grid_size=(50, 50),
        start=(5.0, 5.0),
        goal=(45.0, 45.0),
    )
    print("âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    
    # 2. æ¼”ç¤ºéšæœºè·¯å¾„ï¼ˆå¯¹æ¯”ï¼‰
    random_path_demo(env, demo_dir)
    
    # 3. æ¼”ç¤ºè´ªå¿ƒè·¯å¾„ï¼ˆå¯¹æ¯”ï¼‰
    greedy_path_demo(env, demo_dir)
    
    # 4. è®­ç»ƒRLæ™ºèƒ½ä½“ï¼ˆå¦‚æœSB3å¯ç”¨ï¼‰
    if SB3_AVAILABLE:
        model = train_rl_agent(env, algorithm='SAC', total_timesteps=5000)
        
        if model:
            # 5. æµ‹è¯•æ™ºèƒ½ä½“
            test_agent(model, env, demo_dir, num_episodes=3)
            
            # 6. ä¿å­˜æ¨¡å‹
            model_path = demo_dir / "rl_path_model"
            model.save(str(model_path))
            print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    else:
        print("\nâš ï¸  Stable-Baselines3 æœªå®‰è£…ï¼Œè·³è¿‡è®­ç»ƒæ­¥éª¤")
        print("   å¯ä»¥è¿è¡Œ: pip install \"stable-baselines3[extra]\"")
    
    print("\n" + "=" * 60)
    print("âœ… Demo å®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ“ è¯´æ˜:")
    print("   - è¿™ä¸ªdemoæ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¼šä¿®æ”¹ä½ çš„é¡¹ç›®ä»£ç ")
    print("   - ç”Ÿæˆçš„å›¾åƒä¿å­˜åœ¨ demo/ ç›®å½•")
    print("   - å¯ä»¥å¯¹æ¯”éšæœºè·¯å¾„ã€è´ªå¿ƒè·¯å¾„å’ŒRLä¼˜åŒ–è·¯å¾„çš„æ•ˆæœ")


if __name__ == "__main__":
    main()

